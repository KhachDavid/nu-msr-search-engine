<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2025-09-12 Fri 13:21 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Who Stole My Pen?</title>
<meta name="author" content="Matthew Elwin" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="./pubme.css" type="text/css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./index.html"> HOME </a>
</div><div id="content" class="content">
<header>
<h1 class="title">Who Stole My Pen?</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#overview-1-P-1">Overview</a></li>
<li><a href="#setup-1-P-4">Setup</a>
<ul>
<li><a href="#software-2-P-1">Software</a></li>
<li><a href="#physical-2-P-1">Physical</a></li>
</ul>
</li>
<li><a href="#pen-recognition-1-P-1">Pen Recognition</a>
<ul>
<li><a href="#alignment-2-P-1">Alignment</a></li>
<li><a href="#recording-2-P-1">Recording</a></li>
<li><a href="#image-pipeline-2-P-1">Image Pipeline</a></li>
</ul>
</li>
<li><a href="#robot-control-1-P-1">Robot Control</a>
<ul>
<li><a href="#caution-2-P-1">Caution</a></li>
<li><a href="#first-moves-2-P-1">First Moves</a></li>
<li><a href="#next-moves-2-P-1">Next Moves</a></li>
<li><a href="#useful-functions-2-P-1">Useful Functions</a></li>
<li><a href="#workarounds-2-P-1">Workarounds</a></li>
</ul>
</li>
<li><a href="#pen_challenge_calibration">Extrinsic Calibration</a>
<ul>
<li><a href="#pen_challenge_calibration_setup">Calibration Setup</a></li>
<li><a href="#calibration-math-2-P-1">Calibration Math</a></li>
<li><a href="#calibration-procedure-2-P-1">Calibration Procedure</a></li>
</ul>
</li>
<li><a href="#robot-control-1-P-2">Robot Control</a>
<ul>
<li><a href="#multi-processing-2-P-1">Multi-Processing</a></li>
</ul>
</li>
<li><a href="#relay-mode-1-P-1">Relay Mode</a></li>
<li><a href="#hints-1-P-2">Hints</a></li>
<li><a href="#other-references-1-P-1">Other References</a></li>
<li><a href="#bibliography-1-P-1">Bibliography</a></li>
</ul>
</div>
</nav>
<div id="outline-container-overview-1-P-1" class="outline-2">
<h2 id="overview-1-P-1">Overview</h2>
<div class="outline-text-2" id="text-org25784b0">
<p>
We've all done it. "Borrowed" a pen and then lost it.
Let's see if we can get PincherX 100 to hold on to it for us!
</p>

<p>
The goal is enable the PincherX 100 to grab a pen that is held in front of it.
</p>
</div>
</div>

<div id="outline-container-setup-1-P-4" class="outline-2">
<h2 id="setup-1-P-4">Setup</h2>
<div class="outline-text-2" id="text-setup-1-P-4">
</div>
<div id="outline-container-software-2-P-1" class="outline-3">
<h3 id="software-2-P-1">Software</h3>
<div class="outline-text-3" id="text-orgfc8326a">
<p>
The software setup was completed as part of <a href="./computer_setup.html">Computer Setup</a>.
</p>
</div>
</div>
<div id="outline-container-physical-2-P-1" class="outline-3">
<h3 id="physical-2-P-1">Physical</h3>
<div class="outline-text-3" id="text-orgbecdea6">
<p>
The physical setup for this project requires the Trossen PincherX 100 and the Intel Realsense D435i.
Here are some tips for how to locate the hardware.
</p>

<ol class="org-ol">
<li>The field of view of the RealSense should substantially overlap with part of the PincherX's workspace</li>
<li>Try to keep the PincherX away from walls and valuables; (you never know when it will go rogue!)</li>
<li>I've found the most compact setup is to offset the D435i 90 degrees from the front of the PincherX</li>
<li>Part of the challenge is to place the robot and sensor in appropriate locations.  Depending on how you do this,
the algorithm may have different parameters.</li>
<li>Remember to plug the power cable into the PincherX 100.</li>
<li>The RealSense has a tripod you can use to keep it in one location (also, the legs of the tripod extend, if needed).
<ul class="org-ul">
<li>An alternative is to place the RealSense on top of it's box.</li>
</ul></li>
</ol>
</div>
</div>
</div>

<div id="outline-container-pen-recognition-1-P-1" class="outline-2">
<h2 id="pen-recognition-1-P-1">Pen Recognition</h2>
<div class="outline-text-2" id="text-org6982213">
<p>
Use the RealSense to measure the 3D location of your purple MSR pen.
This guide provides steps that I consider to be the simplest that can minimally accomplish the accuracy needed
to complete the challenge. However, feel free to use any other techniques that you see fit.
</p>

<p>
The approach is to use classical computer vision techniques on the RGB image to locate the pen in 2D space. We then
align the Depth map (an image where each pixel is a depth measurement) to the RGB image and use the pen location as a mask to get the 3D information.
Finally, we find the centroid (and possibly other geometric moments) of the pen. This information will be fed into
a controller that will enable the robot to grab the pen.
</p>
</div>


<div id="outline-container-alignment-2-P-1" class="outline-3">
<h3 id="alignment-2-P-1">Alignment</h3>
<div class="outline-text-3" id="text-orgb7fa8b6">
<ol class="org-ol">
<li>Alignment is the process of placing two images, taken from different perspectives into the same reference frame</li>
<li>In this case, we collect two images: a depth map and a Red, Green, Blue (RGB) image
<ul class="org-ul">
<li>The Depth Map is an <code>NxM</code> array, where each "pixel" is a 16-bit integer proportional to the distance from the camera to the nearest object.</li>
<li>The RGB image is an <code>NxMx3</code> array, where each <code>NxM</code> slice is a different R, G, or B color (be careful, OpenCV often uses BGR instead of RGB ordering for colors!)</li>
</ul></li>
<li>This <a href="https://github.com/IntelRealSense/librealsense/blob/master/wrappers/python/examples/align-depth2color.py">Alignment Example</a> from Intel demonstrates how to do several tasks and is a good starting point for your project
<ol class="org-ol">
<li>Stream depth and RGB images from the camera</li>
<li>Convert the depth "pixel" value into meters
<ul class="org-ul">
<li>The example finds a scaling factor and manually converts. This scaling factor is useful if you want to, for example, convert the entire depth-image to meters.</li>
<li>The depth image also has a <a href="https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.depth_frame.html#pyrealsense2.depth_frame.get_distance">get_distance</a> function that does the conversion for a single pixel.</li>
</ul></li>
<li>Remove objects that are farther than a distance threshold away</li>
<li>Align both images</li>
<li>Process the RGB image with OpenCV</li>
<li>Display the results with OpenCV</li>
</ol></li>
</ol>
</div>
<div id="outline-container-design-notes-3-P-1" class="outline-4">
<h4 id="design-notes-3-P-1">Design Notes</h4>
<div class="outline-text-4" id="text-org3607939">
<ol class="org-ol">
<li>What follows are some notes on how to think about designing and organizing your code:</li>
<li>A good strategy, after running and understanding the <a href="https://github.com/IntelRealSense/librealsense/blob/master/wrappers/python/examples/align-depth2color.py">Alignment Example</a> is to divide it into the components that you need and encapsulate them in a class.</li>
<li>For example, there are a few steps required to capture images:
<ul class="org-ul">
<li>Setup streaming (this can be done, for example, in the <code>__init__</code> method).</li>
<li>Capture a frame, get the depth image, etc. (these are class methods)</li>
<li>Stop the pipeline and cleanup.</li>
<li>By creating a class you can separate two concerns: the actual mechanics of obtaining a frame, and the continuous processing of the frame (as done by the <code>while</code> loop in the example)</li>
</ul></li>
<li>In python the <code>with</code> statement is useful for using objects that must initialize a resource (e.g., a file) and then clean it up when finished.
<ul class="org-ul">
<li>With statements work with  <a href="https://docs.python.org/3/reference/datamodel.html#context-managers">Context Managers</a> to enter and exit a <i>runtime context</i></li>
<li>In this case, entering the context can be seen as setting up the streaming pipeline, and leaving the context shuts the pipeline down.</li>
<li>The general idea is that, even if code within the <code>with</code> block throws an exception, the cleanup code will run automatically (without requiring <code>try-finally</code>)</li>
</ul></li>
<li>A good way to test this class is to use it to write a script that displays the aligned depth and RGB images to the user.</li>
<li>The OpenCV GUI will not update unless events are processed. Two functions are available to process events:
<ul class="org-ul">
<li><code>cv2.waitKey</code> will wait for a key to be pressed for some specified number of milliseconds (the minimum is 1). Run <code>help("cv2.waitKey")</code> for more information.</li>
<li><code>cv2.pollKey</code> updates the event loop and checks if a key was pressed since it's last invocation (so it does not wait). Run <code>help("cv2.pollKey")</code> for more information.</li>
<li>It is necessary to call one of these functions periodically in order to have a responsive GUI: the most straight-forward way is for the bulk of your
program to be designed to operate in a loop where one of these functions are called once per iteration.</li>
</ul></li>
</ol>
</div>
</div>


<div id="outline-container-resources-3-P-1" class="outline-4">
<h4 id="resources-3-P-1">Resources</h4>
<div class="outline-text-4" id="text-org003ec99">
<ol class="org-ol">
<li><a href="https://intelrealsense.github.io/librealsense/python_docs/">Realsense Python API Documentation</a></li>
<li><a href="https://github.com/IntelRealSense/librealsense/tree/master/wrappers/python/examples">PyRealSense Examples</a></li>
</ol>
</div>
</div>
</div>

<div id="outline-container-recording-2-P-1" class="outline-3">
<h3 id="recording-2-P-1">Recording</h3>
<div class="outline-text-3" id="text-orgef56146">
<ul class="org-ul">
<li>Being able to record data and play it back is useful when testing an image pipeline because it ensures that you are using the same data each time.</li>
<li>To record data, use <a href="https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.config.html#pyrealsense2.config.enable_record_to_file">config.enable_record_to_file(filename)</a></li>
<li>To play back the data, use the <a href="https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.config.html#pyrealsense2.config.enable_device_from_file">config.enable_device_from_file(filename)</a></li>
<li>You may wish to provide command-line arguments to easily switch between recording and playback: the <a href="https://docs.python.org/3/library/argparse.html">argparse</a> library may be helpful.</li>
</ul>
</div>

<div id="outline-container-design-notes-3-P-2" class="outline-4">
<h4 id="design-notes-3-P-2">Design Notes</h4>
<div class="outline-text-4" id="text-org1649653">
<ol class="org-ol">
<li>By encapsulating the camera streaming in a class, it is easier to write a program that lets a user choose between playing from a recording or streaming live data.</li>
<li>The recording and playback functionalities are useful to implement before moving forward with the image pipeline.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-image-pipeline-2-P-1" class="outline-3">
<h3 id="image-pipeline-2-P-1">Image Pipeline</h3>
<div class="outline-text-3" id="text-orgf735208">
<ol class="org-ol">
<li>We will use OpenCV to process the images from the Realsense.</li>
<li>A complete list of python tutorials is <a href="https://docs.opencv.org/4.6.0/d6/d00/tutorial_py_root.html">here</a>.</li>
<li>It is useful during debug to visualize each step in an image processing pipeline
<ul class="org-ul">
<li>Create a new window using <code>cv2.imshow("name", image)</code> or <code>cv2.newWindow()</code></li>
<li>First, get something reasonable, than worry about refining your object detection.</li>
</ul></li>
<li>You can use any image processing methods that you want.
<ul class="org-ul">
<li>I've listed some techniques to get you started</li>
<li>The end goal of this pipeline is to output the centroid of the pen</li>
</ul></li>
<li>The Depth map is calibrated by Intel and you can directly convert to real units (as the alignment example does)
<ul class="org-ul">
<li>You can use camera calibration and the known positions of the robot and RealSense to solve for the other dimensions.</li>
<li>The Realsense's built-in calibration is not great so for most projects you should do a calibration yourself: however, it is good enough for this project.</li>
<li>More information on calibration is under <a href="#orga16dc43">Useful Techniques</a></li>
</ul></li>
<li>The output of the image pipeline should be the \((x,y,z)\) location, in meters, relative to the camera frame.
<ul class="org-ul">
<li>Be sure to display or print this information, as it will be needed later.</li>
</ul></li>
</ol>
</div>
<div id="outline-container-useful-techniques-3-P-1" class="outline-4">
<h4 id="useful-techniques-3-P-1">Useful Techniques</h4>
<div class="outline-text-4" id="text-orga16dc43">
<ol class="org-ol">
<li><a href="https://docs.opencv.org/4.6.0/da/d6a/tutorial_trackbar.html">Trackbars</a>
<ul class="org-ul">
<li>Trackbars let you set parameters in a GUI in real time.  Useful when trying to tweak parameters when processing an image.</li>
<li>For each step of the pipeline, you can create trackbars to control the basic parameters and experiment until you find ones that work</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/df/d9d/tutorial_py_colorspaces.html">Convert between color spaces</a>
<ul class="org-ul">
<li>When distinguishing objects based on colors, it is often useful to convert to the Hue, Saturation, Value (HSV) colorspace.
<ul class="org-ul">
<li>In this space, the part of the light that contributes to color is relatively isolated from other factors such as brightness</li>
</ul></li>
<li>In OpenCV, The Value (V) is between 0 and 255, Saturation (S) is between 0 and 255, and the Hue is between 0 and 180 (<a href="https://docs.opencv.org/4.6.0/de/d25/imgproc_color_conversions.html#color_convert_rgb_hsv">details</a>)</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/da/d97/tutorial_threshold_inRange.html">Thresholding</a>
<ul class="org-ul">
<li>Creates a mask image, with either black or white pixels depending on if the pixel in the original image met some criteria</li>
<li><a href="https://docs.opencv.org/4.6.0/d7/d4d/tutorial_py_thresholding.html">More about Thresholding</a></li>
<li>Use thresholding to mask off the pixels that are part of the pen.</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/d0/d86/tutorial_py_image_arithmetics.html">Arithmetic Operations</a>
<ul class="org-ul">
<li>Images can be combined using arithmetic operations</li>
<li>Bitwise operations are especially useful for combining image masks, for example to see the RGB image only in the unmasked areas</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/d4/d73/tutorial_py_contours_begin.html">How to find contours</a>
<ul class="org-ul">
<li>A contour is a closed geometric shape in an image</li>
<li>Getting the contour from the masked image of a pen gives you geometric information about the pen</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/dd/d49/tutorial_py_contour_features.html">How to get properties of contours</a>
<ul class="org-ul">
<li>Useful for getting properties of contours, like area or the centroid</li>
</ul></li>
<li><a href="https://docs.opencv.org/4.6.0/d4/d13/tutorial_py_filtering.html">Filtering an image</a>
<ul class="org-ul">
<li>Computer vision sensors are noisy, these filters can help</li>
<li><a href="https://docs.opencv.org/4.6.0/db/df6/tutorial_erosion_dilatation.html">Additional image filters</a></li>
<li><a href="https://docs.opencv.org/4.6.0/d3/dbe/tutorial_opening_closing_hats.html">More Filters</a></li>
</ul></li>
<li><a href="https://github.com/IntelRealSense/librealsense/wiki/Projection-in-RealSense-SDK-2.0">Intel Documentation</a> describes various camera parameters and how to convert between pixel and real-world coordinates.
<ul class="org-ul">
<li>The approach I suggest uses a 2D color image and a depth map, but you may also be able to complete this project using point-clouds (a collection of 3-dimensional points).</li>
<li><p>
<a href="https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.html">rs2_deproject_pixel_to_point</a> can be used to to convert the depth map and pixel coordinates into real coordinates
</p>
<div class="org-src-container">
<pre class="src src-python">rs.rs2_deproject_pixel_to_point(intrinsics, [px, py], depth)
<span class="org-comment-delimiter"># </span><span class="org-comment">intrinsics - the intrinsic parameters
</span><span class="org-comment-delimiter"># </span><span class="org-comment">(px, py) - the pixel coordinates
</span><span class="org-comment-delimiter"># </span><span class="org-comment">depth - the depth in meters
</span><span class="org-comment-delimiter"># </span><span class="org-comment">returns the x,y, and z coordinates in meters as a list</span>
</pre>
</div></li>
<li><p>
The realsense knows its own <a href="https://intelrealsense.github.io/librealsense/python_docs/_generated/pyrealsense2.intrinsics.html?highlight=intrinsics">intrinsic</a> parameters. You can get them with the python api
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">cfg</span> <span class="org-operator">=</span> pipeline.start(config)
<span class="org-variable-name">profile</span> <span class="org-operator">=</span> cfg.get_stream(rs.stream.color)
<span class="org-variable-name">intr</span> <span class="org-operator">=</span> profile.as_video_stream_profile().get_intrinsics()
</pre>
</div></li>
<li>Given the location of the pincherX relative to the camera frame, you can convert camera (x,y,z) coordinates into
cylindrical coordinates centered at the base frame of the pincherX</li>
<li>For some theoretical knowledge about camera calibration
<ul class="org-ul">
<li><a href="https://docs.opencv.org/4.6.0/d4/d94/tutorial_camera_calibration.html">Camera Calibration</a> explains how to convert pixel coordinates to real coordinates</li>
<li><a href="https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html">Calibration</a> shows how to calibrate using OpenCV.</li>
</ul></li>
</ul></li>
<li>The depth map can be used to remove parts of the image that are clearly not in the workspace.
<ul class="org-ul">
<li>This technique is only available because we have depth information: it can reduce the burden of tuning the detector</li>
<li>The Alignment example code does this step.</li>
</ul></li>
<li>For debugging it is sometimes useful to be able to <a href="https://docs.opencv.org/4.6.0/dc/da5/tutorial_py_drawing_functions.html">draw</a></li>
</ol>
</div>
</div>
<div id="outline-container-design-notes-3-P-3" class="outline-4">
<h4 id="design-notes-3-P-3">Design Notes</h4>
<div class="outline-text-4" id="text-org4ba2441">
<ol class="org-ol">
<li>The ultimate goal of the pipeline is to get the 3D pen location from the RGB image and depth map.</li>
<li>Do not worry about precision and accuracy at this stage: just get a rough reasonable version working.
<ul class="org-ul">
<li>You do not know how accurate and precise the tracking needs to be to grab the pen until you work on the control</li>
<li>You can always come back and refine the tracking pipeline once it appears as a problem while attempting the control.</li>
</ul></li>
<li>It may be helpful to give yourself the option to easily toggle debugging information on or off without modifying the code:
<ul class="org-ul">
<li>It is useful to be able to visualize each step in the pipeline</li>
<li>It is useful to be able to visualize important features on top of the original RGB image (such as the contour location).</li>
</ul></li>
<li>It may be useful to write a class to manage trackbar settings</li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-robot-control-1-P-1" class="outline-2">
<h2 id="robot-control-1-P-1">Robot Control</h2>
<div class="outline-text-2" id="text-org4779329">
<p>
We will use the <code>interbotix_xs_toolbox</code> to control the robot.
Although the arm can be fully controlled with ROS 2, we will use only minimally use it during the hackathon,
instead relying on the simpler <code>python</code> API (which uses ROS 2 under the hood).
</p>
</div>


<div id="outline-container-caution-2-P-1" class="outline-3">
<h3 id="caution-2-P-1">Caution</h3>
<div class="outline-text-3" id="text-org1deb2af">
<ul class="org-ul">
<li>See <a href="https://docs.trossenrobotics.com/interbotix_xsarms_docs/specifications/px100.html">Specifications</a> for a list of joint ids and their limits</li>
<li>If a joint limit is exceeded, it will hit a hard-stop and the motor will turn off and flash red.</li>
<li>If a motor is flashing red, you should end the <code>ros2 launch</code> and power-cycle the robot</li>
<li>The default <code>sleep</code> position</li>
<li>See <a href="https://www.trossenrobotics.com/docs/interbotix_xsarms/troubleshooting/index.html">Troubleshooting Notes</a></li>
<li>Using the robot requires the <code>interbotix</code> workspace setup to be run in each terminal window you are using.</li>
</ul>
</div>
</div>
<div id="outline-container-first-moves-2-P-1" class="outline-3">
<h3 id="first-moves-2-P-1">First Moves</h3>
<div class="outline-text-3" id="text-org5f00cea">
<ol class="org-ol">
<li><p>
To start the arm, open a new terminal window and run the following commands (see <a href="computer_setup.html#interbotix_setup">Interbotix Setup Instructions</a> for details).
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-builtin">source</span> ws/interbotix/install/setup.bash
ros2 launch interbotix_xsarm_control xsarm_control.launch.py robot_model:=px100
</pre>
</div></li>
<li>An <code>rviz</code> window showing the location of the arm should open.</li>
<li>To Stop running the arm, press <code>C-c</code> in the terminal window.
<ul class="org-ul">
<li>When you stop the robot the motors loose power and the arm may crash to the ground.</li>
<li>To prevent damage, always return the robot to its sleep position prior to stopping it or hold the robot and guide it down gently</li>
</ul></li>
<li>Keep this terminal open until you are ready to stop using the robot. It must be open and running for you to run the robot</li>

<li>I have provided (very minimal!) sample code for using the interbotix python sdk.
<ul class="org-ul">
<li>See the comments in <a href="https://github.com/Interbotix/interbotix_ros_toolboxes/blob/humble/interbotix_xs_toolbox/interbotix_xs_modules/interbotix_xs_modules/xs_robot/arm.py">interbotix_xs_modules/arm.py</a> and the <a href="https://docs.trossenrobotics.com/interbotix_xsarms_docs/python_ros_interface.html">official documentation</a></li>
<li>This code only works if the previous <code>ros2 launch</code> command is still running in the background</li>
<li>Especially when moving the robot for the first time, make sure it is clear of any potential obstacles</li>
<li>The code below lets you toggle between the sleep and home position</li>
<li><p>
You must <code>source ws/interbotix/install/setup.bash</code> from any terminal where you want to run Interbotix code
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> interbotix_xs_modules.xs_robot.arm <span class="org-keyword">import</span> InterbotixManipulatorXS
<span class="org-keyword">from</span> interbotix_common_modules.common_robot.robot <span class="org-keyword">import</span> robot_shutdown, robot_startup
<span class="org-comment-delimiter"># </span><span class="org-comment">The robot object is what you use to control the robot
</span><span class="org-variable-name">robot</span> <span class="org-operator">=</span> InterbotixManipulatorXS(<span class="org-string">"px100"</span>, <span class="org-string">"arm"</span>, <span class="org-string">"gripper"</span>)

robot_startup()
<span class="org-variable-name">mode</span> <span class="org-operator">=</span> <span class="org-string">'h'</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">Let the user select the position
</span><span class="org-keyword">while</span> mode <span class="org-operator">!=</span> <span class="org-string">'q'</span>:
    <span class="org-variable-name">mode</span><span class="org-operator">=</span><span class="org-builtin">input</span>(<span class="org-string">"[h]ome, [s]leep, [q]uit "</span>)
    <span class="org-keyword">if</span> mode <span class="org-operator">==</span> <span class="org-string">"h"</span>:
        robot.arm.go_to_home_pose()
    <span class="org-keyword">elif</span> mode <span class="org-operator">==</span> <span class="org-string">"s"</span>:
        robot.arm.go_to_sleep_pose()

robot_shutdown()
</pre>
</div></li>
<li>To see what other commands you can run, look at the documentation linked above.</li>
<li>Keep in mind, the robot has only 4 degrees of freedom (DOF). This means that not all end-effector poses are possible.</li>
</ul></li>

<li>Dive-weight positioning
<ul class="org-ul">
<li>The dive weight helps stabilize the robot</li>
<li>However, it is a bit large and can interfere with the sleep position</li>
<li>Try to position the weight so that the arm can reach the sleep position and go back to the home position</li>
<li>Run the example code to move the arm between sleep and home several times.
<ul class="org-ul">
<li>If any of the motors flash red when going to the sleep position, this means your weight is interfering. you will need
to power cycle the robot and try again</li>
</ul></li>
<li>As the robot moves, you can watch it move in the RViz window as well</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-next-moves-2-P-1" class="outline-3">
<h3 id="next-moves-2-P-1">Next Moves</h3>
<div class="outline-text-3" id="text-orgabede13">
<p>
Here are some recommended steps to get the robot capturing the pen.
</p>
<ol class="org-ol">
<li>Start from my example code and add some testing modes to perform the following actions:
<ol class="org-ol">
<li>Open and close the grippers.</li>
<li>Move forward or backward by small distance (relative or absolute?)</li>
<li>Move up and down by a small distance (relative or absolute?)</li>
<li>Rotate arm about the base by a small distance (relative or absolute?)</li>
</ol></li>
<li>Use these modes to get a feel for the workspace limitations of the robot.
<ul class="org-ul">
<li>There are many limitations to the reachable areas of the robot's workspace.</li>
<li>Singularities: where the robot geometry causes it to lose a degree of freedom, preventing it from moving in certain directions</li>
<li>Joint Limits: mechanical stops and limitations that prevent a joint from rotating more than a certain amount</li>
<li>Torque Limits: at some locations the motors may not have enough torque to move the weight of the robot.
<ul class="org-ul">
<li>At these locations, it is possible to command a move that causes a motor to over-torque and shutdown</li>
<li>The robot will then fall.</li>
<li>Industrial robots typically have more than enough power to lift themselves so this would not be an issue with them.</li>
</ul></li>
<li>If the planner fails to compute the trajectory, it means that the algorithm could not find a feasible path.
<ul class="org-ul">
<li>A path might be infeasible due to geometry but it also might be infeasible due to time</li>
<li>Most movement functions have various parameters related to the timing of the action (for example, if commanding the robot to a position how long should
it take for the robot to arrive).</li>
<li>Timing is important: large displacements require more time or the acceleration and velocity limits are exceeded. Small displacements can only be done
so slowly due to friction and other losses in the system.</li>
</ul></li>
</ul></li>
<li>After each movement, the program should print out the end-effector pose and joint values.</li>
</ol>
</div>
</div>
<div id="outline-container-useful-functions-2-P-1" class="outline-3">
<h3 id="useful-functions-2-P-1">Useful Functions</h3>
<div class="outline-text-3" id="text-org274c53a">
<ol class="org-ol">
<li><code>robot.arm.set_ee_cartesian_trajectory</code>
<ul class="org-ul">
<li>Move the end-effector along a straight-line path</li>
</ul></li>
<li><code>robot.arm.get_joint_commands</code>
<ul class="org-ul">
<li>Get the commanded joint positions of the robot</li>
<li>The <code>robot.core.robot_get_joint_states</code> function gets the actual state of the joints.
<ul class="org-ul">
<li>This is better information for our purposes, but it is also lower-level and not needed (<code>get_joint_commands</code> is sufficient)</li>
<li>It is listed here for reference.</li>
</ul></li>
</ul></li>
<li><code>robot.arm.set_joint_positions</code>
<ul class="org-ul">
<li>Set joint positions of the robot</li>
</ul></li>
<li><code>set_single_joint_position</code>
<ul class="org-ul">
<li>Set the position of a single joint</li>
<li>The joint names (from base to end-effector) are <code>waist</code>, <code>shoulder</code>, <code>elbow</code>, <code>wrist_angle</code></li>
</ul></li>
<li><code>robot.arm.get_ee_pose()</code> Get's the pose of the end effector
<ul class="org-ul">
<li>This is a transformation matrix of the end effector relative to the base of the robot</li>
<li>The last column contains the x, y, and z positions in rows 0-2 respectively</li>
</ul></li>
<li><p>
To compute the end-effector given the joint states, you
can use the <a href="https://github.com/NxRLab/ModernRobotics">modern_robotics</a> library directly:
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> modern_robotics <span class="org-keyword">as</span> mr
<span class="org-variable-name">joints</span> <span class="org-operator">=</span> [waist, shoulder, elbow, waist_angle] <span class="org-comment-delimiter"># </span><span class="org-comment">pseudocode, use your own values
</span><span class="org-variable-name">T</span> <span class="org-operator">=</span> mr.FKinSpace(robot.arm.robot_des.M, robot.arm.robot_des.Slist, joints)
[<span class="org-variable-name">R</span>, <span class="org-variable-name">p</span>] <span class="org-operator">=</span> mr.TransToRp(T) <span class="org-comment-delimiter"># </span><span class="org-comment">get the rotation matrix and the displacement</span>
</pre>
</div></li>
<li>Some of the above commands have an option to make them <i>blocking</i> or <i>non-blocking</i>
<ul class="org-ul">
<li>When blocking, the function will wait until the action is complete before returning (this is the default)</li>
<li>When <i>non-blocking</i>, the function returns while the robot is still moving. Movement can then be updated by issuing another command</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-workarounds-2-P-1" class="outline-3">
<h3 id="workarounds-2-P-1">Workarounds</h3>
<div class="outline-text-3" id="text-orgda89471">
<ul class="org-ul">
<li>The following are workarounds either to undocumented or incorrect behavior in the <code>interbotix</code> library</li>
<li>Narrowing the problem down to a minimal example and reporting it on github can likely lead to a successful pull-request.</li>
<li><code>robot.arm.set_trajectory_time(2, 0.3)</code> resets the trajectory timing to the defaults
<ul class="org-ul">
<li>The current behavior of the library sometimes causes moves like <code>robot.arm.go_to_home_pose()</code> to
use the settings from the last call to <code>set_ee_cartesian_trajectory</code>, which can lead to some unexpected motions</li>
</ul></li>

<li><code>robot_startup()</code> starts a ROS 2 node in a separate thread to process updates to the joint positions.
As far as I can tell, the need to use this function is not documented anywhere, but without it the robot's position is
only updated when calling some (but not all) movement commands.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-pen_challenge_calibration" class="outline-2">
<h2 id="pen_challenge_calibration">Extrinsic Calibration</h2>
<div class="outline-text-2" id="text-pen_challenge_calibration">
<p>
To control the robot using data from the Realsense, it is necessary to know where the Realsense is relative to the robot.
To be done properly this type of <i>hand-eye calibration</i> must use measurements from the camera, because it is extremely difficult
to measure the physical location of the camera's center (which is inside the camera itself and cannot be measured externally).
</p>


<p>
The goal of the calibration is to find the transformation (i.e., the translation and rotation) between the camera's coordinate frame \(\{c\}\)
and robot's coordinate frame \(\{r\}\).
</p>

<p>
There are two approaches:
</p>
<ol class="org-ol">
<li>Hand Tuning: take a measurement, and adjust until the system works.
<ul class="org-ul">
<li>This method is hack, (but this is a <i>hackathon</i>).</li>
<li>Measurements are only approximations so tuning is required.</li>
<li>There are technically 6 Degrees of freedom that describe the transformation between camera and robot (i.e., six inter-related parameters to tune)</li>
<li>Making some simplifying assumptions can reduce the number of parameters (e.g., assume the camera is flat relative to the ground)</li>
<li>Inaccurate, not repeatable, but can get the job done in a pinch (if the goal is a one-time demo).</li>
</ul></li>

<li>Calibration procedure: collect a bunch of data points where the end-effector is at a known location relative to the robot base and the camera.
<ul class="org-ul">
<li>Requires more math and coding than hand-tuning</li>
<li>Once implemented and working, calibration can be automated</li>
<li>Little or no hand-tuning required</li>
<li>Most accurate method (properly calibrated cameras can be highly precise)</li>
<li>Usually requires some infrastructure (such as a calibration fixture). We will use the pen!</li>
</ul></li>
</ol>
</div>

<div id="outline-container-pen_challenge_calibration_setup" class="outline-3">
<h3 id="pen_challenge_calibration_setup">Calibration Setup</h3>
<div class="outline-text-3" id="text-pen_challenge_calibration_setup">
<p>
Regardless of the method you choose, understanding the assumptions you are making and having a good picture of the coordinate frames in question
is essential for debugging.
</p>
</div>

<div id="outline-container-drawing-and-assumptions-3-P-1" class="outline-4">
<h4 id="drawing-and-assumptions-3-P-1">Drawing and Assumptions</h4>
<div class="outline-text-4" id="text-orgc35e259">
<ol class="org-ol">
<li>Make sure you understand the coordinate systems used by the camera and the robot.
<ul class="org-ul">
<li>You can discover this through experimentation or by reading the <a href="https://dev.intelrealsense.com/docs/projection-in-intel-realsense-sdk-20">documentation</a></li>
<li>You should develop a theoretical understanding from the documentation that you then confirm with experiments</li>
</ul></li>
<li>Make a drawing of the coordinate systems involved. It should include
<ol class="org-ol">
<li>A top-down view, showing the camera and robot axes.</li>
<li>A side view showing the camera and robot's vertical axis and the \(d_z\) offset between them.</li>
</ol></li>
<li>Once you have a calibration use <code>matplotlib</code> to plot the top-down view of the coordinate frames and check if the result is reasonable.
<ul class="org-ul">
<li>This quick verification does not require the robot or camera, just the calibration and data</li>
<li>Doing a quick drawing in <code>matplotlib</code> can greatly speed up debugging</li>
<li>Check that the generated drawing matches with your sketch and that the distances are reasonable</li>
</ul></li>
</ol>
</div>
</div>
</div>

<div id="outline-container-calibration-math-2-P-1" class="outline-3">
<h3 id="calibration-math-2-P-1">Calibration Math</h3>
<div class="outline-text-3" id="text-org5ab7bdd">
<p>
Here I provide practical guide for the calibration math. This section is based on the notes here: <a href="#citeproc_bib_item_1">[1]</a>)
</p>

<ol class="org-ol">
<li>Assume we have a set of \(n\) points:
<ul class="org-ul">
<li>\(P_i \in \mathbb{R}^3, i = 1 \text{ to } n\) are the points in the camera frame</li>
<li>\(Q_i \in \mathbb{R}^3, i = 1 \text{ to } n\) are the same points in the robot frame</li>
</ul></li>

<li>Find the centroid of the points:
<ul class="org-ul">
<li>\(\bar{P} = \frac{1}{n}\sum^n_i P_i\)</li>
<li>\(\bar{Q} = \frac{1}{n}\sum^n_i Q_i\)</li>
</ul></li>
<li>Subtract the centroid from each point:
<ul class="org-ul">
<li>\(\bar{P}_i = P_i - \bar{P}\)</li>
<li>\(\bar{Q}_i = Q_i - \bar{Q}\)</li>
<li>The centroid of both point sets are at the same location in space.</li>
</ul></li>
<li>If the points in one set undergo a pure rotation about their centroid, they should line up with the points in the other set exactly (except for noise).
<ul class="org-ul">
<li>The immediate is to find a rotation matrix \(R \in \mathbb{R}^{3\times3}\) such that \(\bar{Q}_i = R \bar{P}_i \text{ for } i = 1 \text { to } n\).</li>
<li>An algorithm to find \(R\) is the <a href="https://en.wikipedia.org/wiki/Kabsch_algorithm">Kabsch Algorithm</a>, which is implemented in scipy as <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.align_vectors.html">scipy.spatial.transform.Rotation.align_vectors</a></li>
<li>The algorithm takes two sets of points and finds the rotation between them (assuming they only differ by a rotation and some noise).</li>
</ul></li>
<li>After finding \(R\) the goal is to find the translation \(t\) between the point-sets:
<ul class="org-ul">
<li>Converting from a point \(P\) in the camera frame to a point \(Q\) in the robot frame is done with \(Q = RP + t\).</li>
<li>Once \(R\) is known then \(t\) can be found using the conversion formula and the centroids (or any two corresponding points)</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-calibration-procedure-2-P-1" class="outline-3">
<h3 id="calibration-procedure-2-P-1">Calibration Procedure</h3>
<div class="outline-text-3" id="text-org5cf6caf">
<p>
Write a script that performs the calibration procedure:
</p>
<ol class="org-ol">
<li>The script should be run while the robot is holding a pen</li>
<li>The robot should move to a few pre-set positions to gather the necessary data</li>
<li>The script then computes and outputs the calibration parameters</li>
<li>The robot moves to a few test locations and the camera coordinates are converted to the robot coordinates and compared</li>
</ol>
</div>

<div id="outline-container-design-notes-3-P-4" class="outline-4">
<h4 id="design-notes-3-P-4">Design Notes</h4>
<div class="outline-text-4" id="text-org1239085">
<ol class="org-ol">
<li>There is an offset between where the pen is detected and the center of the robot's end-effector frame:
this offset can be manually accounted for.</li>
<li>It makes sense to have a class that can handle the calibration math in a way that is agnostic to how the data was actually collected.
<ul class="org-ul">
<li>It's responsibilities could include:</li>
<li>Saving calibration parameters to a file</li>
<li>Loading calibration parameters</li>
<li>Performing a calibration, given data points</li>
<li>Performing the conversion between coordinate systems</li>
</ul></li>
<li>It is up to you how the waypoints for the calibration procedure are specified and whether they are in end-effector or joint space.</li>
<li>It makes sense to save the data you collected. This way you can test your analysis code without needing to re-collect data each time.</li>
<li>You can also use your existing code to help specify the waypoints.</li>
<li>Calibration can be a bit finicky: you may get different results if you run it multiple times due to the inaccuracy of the detector and how you place and hold the pen.</li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-robot-control-1-P-2" class="outline-2">
<h2 id="robot-control-1-P-2">Robot Control</h2>
<div class="outline-text-2" id="text-org6fbcd4f">
<p>
A basic robot control loop consists of the following steps:
</p>
<ol class="org-ol">
<li>Move Arm to its starting position</li>
<li>Open the grippers</li>
<li>Measure the Pen Location</li>
<li>Turn at the <i>waist</i> until the end-effector is facing the pen</li>
<li>(Optional) Adjust the height of the arm so the grippers are level with the pen
<ul class="org-ul">
<li>To start out, just hold the pen at the correct height!</li>
</ul></li>
<li>Move forward until the pen is inside the grippers</li>
<li>Close the grippers</li>
</ol>

<p>
There are two possible modes of control: you can use either or a combination of both (e.g., closed loop for the angle, open loop for moving forward).
Below are descriptions of both the closed loop and open loop approach to pointing the robot at the pen.
</p>
</div>
<div id="outline-container-closed-loop-3-P-1" class="outline-4">
<h4 id="closed-loop-3-P-1">Closed Loop</h4>
<div class="outline-text-4" id="text-orgfb9b4f8">
<p>
When rotating (this assumes that both the arm and pen's locations are expressed as cylindrical coordinates relative to the robot base):
</p>
<ol class="org-ol">
<li>Measure the pen location</li>
<li>Measure the robot's location</li>
<li>Move based on the locations (e.g., proportionally to the error between the pen and robot angle)</li>
</ol>
</div>
</div>

<div id="outline-container-open-loop-3-P-1" class="outline-4">
<h4 id="open-loop-3-P-1">Open Loop</h4>
<div class="outline-text-4" id="text-orgaa2486e">
<ol class="org-ol">
<li>Given the pen's location, compute the <code>waist</code> angle and move the arm to that angle</li>
<li>Then move forward based on the computed distance.</li>
</ol>
</div>
</div>


<div id="outline-container-design-notes-3-P-5" class="outline-4">
<h4 id="design-notes-3-P-5">Design Notes</h4>
<div class="outline-text-4" id="text-orgc203a93">
<ol class="org-ol">
<li>There may be a few modes of operation as you control the robot (e.g., moving to the pen, closing grippers, etc). It makes sense to track these modes
by assigning a state to each of them. You can use a python <a href="https://docs.python.org/3/library/enum.html">enum</a> to label and track states</li>
<li>If the pen's location is presented as cylindrical coordinates in the robot frame, the computations can be significantly simplified:
<ul class="org-ul">
<li>It may be helpful to refer to your drawing of the coordinate system</li>
</ul></li>
<li>It may be easier to command locations in joint coordinates rather than needing to go through the IK process
<ul class="org-ul">
<li>You can use other tools you have develo</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-multi-processing-2-P-1" class="outline-3">
<h3 id="multi-processing-2-P-1">Multi-Processing</h3>
<div class="outline-text-3" id="text-org00d86c1">
<ul class="org-ul">
<li>You may have noticed (depending on how you implemented your control loop) that when putting the control together, the image data from the camera slows down.</li>
<li>There are a few contributing factors to this slowness
<ol class="org-ol">
<li>Blocking functions: some <code>interbotix</code> functions do not have the option of being non-blocking: thus, while using these functions to move the robot,
the OpenCV window is not updated and images are not read while the Realsense is moving.
<ul class="org-ul">
<li>It is technically possible to complete this task without using any functions that require blocking, but that would amount to essentially re-writing
some of the <code>interbotix</code> library</li>
</ul></li>
<li>When <code>robot_startup</code> is called, the <code>interbotix</code> library runs a thread in the background to read the joint information from the robot
<ul class="org-ul">
<li>Due to the concurrency model of python, only one thread can execute at a time, thus while the joints are updated, your code does not run</li>
<li>It is possible to run code without <code>robot_startup</code> and manually trigger updates to the robot state, however, accomplishing this would
require a deep dive into the <code>interbotix</code> library implementation.</li>
</ul></li>
</ol></li>
<li>Overall, I believe it is possible to get smooth movement and image processing in a single thread, but not without essentially rewriting parts
of the interbotix library and interacting with the robot on a lower-level than what is provided by the Python API
<ul class="org-ul">
<li>While not ideal, the target of this API is for beginner users: when using the robot via ROS 2 you would not have this problem.</li>
</ul></li>
<li>There is, however, a workaround:
<ol class="org-ol">
<li>Use the python <code>multiprocessing</code> library to run the image updating and movement code in separate processes at the same time.</li>
<li>While there are many ways of implementing this, one of the less error prone is to use a <code>multiprocessing.Queue</code> to pass information
between the processes.</li>
</ol></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-relay-mode-1-P-1" class="outline-2">
<h2 id="relay-mode-1-P-1">Relay Mode</h2>
<div class="outline-text-2" id="text-org02eec65">
<p>
If you've gotten this far, the challenge is complete.  But there is always more to do!
</p>

<ol class="org-ol">
<li>The goal is to have your robot be able to hand off the pen to another robot.</li>
<li>Approach this problem with at least one partner.</li>
<li>Each robot does not necessarily need to be running the same code.</li>
<li>The robots do not necessarily need to communicate with each other.</li>
<li>How long of a hand-off chain can you create?</li>
</ol>
</div>
</div>

<div id="outline-container-hints-1-P-2" class="outline-2">
<h2 id="hints-1-P-2">Hints</h2>
<div class="outline-text-2" id="text-orga0e3b8b">
<ol class="org-ol">
<li>I suggest tackling the project in stages
<ul class="org-ul">
<li>Be able to threshold the pen properly</li>
<li>Be able to measure the pen's location in cylindrical coordinates</li>
<li>Be able to move the robot to a given location in cylindrical coordinates</li>
<li>Put everything together with the control loop</li>
</ul></li>
<li>Test as you go.
<ul class="org-ul">
<li>Use the python interpreter to try statements out and iterate quickly on designs</li>
<li>Draw images of each image processing step</li>
<li>Print out data when you need some more information</li>
<li>You can also use the debugger that is built into VSCode</li>
</ul></li>
<li>It will help to draw diagrams of the workspace geometry
<ul class="org-ul">
<li>I gave you two pens: one to use for testing and one to write with!</li>
</ul></li>
<li>You should expect to tune parameters.
<ul class="org-ul">
<li>I recommend saving good values when you find them</li>
<li>Parameters may change if the lighting conditions change</li>
<li>Get some good rough parameters, move on to the next step, and then do some find tuning at the very end.</li>
</ul></li>
</ol>
</div>
</div>


<div id="outline-container-other-references-1-P-1" class="outline-2">
<h2 id="other-references-1-P-1">Other References</h2>
<div class="outline-text-2" id="text-org5b0d233">
<ul class="org-ul">
<li><a href="https://docs.opencv.org/4.6.0/d6/d00/tutorial_py_root.html">OpenCV Python Tutorials</a></li>
<li><a href="https://numpy.org/doc/1.19/">Numpy Manual</a></li>
<li><a href="https://github.com/NxRLab/ModernRobotics">Modern Robotics</a></li>
</ul>
</div>
</div>
<div id="outline-container-bibliography-1-P-1" class="outline-2">
<h2 id="bibliography-1-P-1">Bibliography</h2>
<div class="outline-text-2" id="text-org5080c01">
<style>.csl-left-margin{float: left; padding-right: 0em;}
 .csl-right-inline{margin: 0 0 0 1em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>
    <div class="csl-left-margin">[1]</div><div class="csl-right-inline">N. Ho, “Finding optimal rotation and translation between corresponding 3d points.” Available: <a href="https://nghiaho.com/?page_id=671 (accessed Sep. 01, 2024)">https://nghiaho.com/?page_id=671 (accessed Sep. 01, 2024)</a></div>
  </div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin. </p></p>
</div>
</body>
</html>
