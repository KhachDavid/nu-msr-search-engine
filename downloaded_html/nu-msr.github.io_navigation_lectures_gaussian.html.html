<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2026-01-05 Mon 23:16 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Jointly Gaussian Distributions</title>
<meta name="author" content="Matthew Elwin" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="./../pubme.css" type="text/css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="../index.html"> UP </a>
 |
 <a accesskey="H" href="./../index.html"> HOME </a>
</div><div id="content" class="content">
<header>
<h1 class="title">Jointly Gaussian Distributions</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#overview-1-P-15">Overview</a></li>
<li><a href="#linear-transform-1-P-1">Linear Transform</a>
<ul>
<li><a href="#sketch-of-a-proof-2-P-1">Sketch of a Proof</a></li>
</ul>
</li>
<li><a href="#conditional-distribution-1-P-1">Conditional Distribution</a></li>
<li><a href="#random-number-generation-in-c--1-P-1">Random Number Generation in C++</a>
<ul>
<li><a href="#multivariate-gaussian-2-P-1">Multivariate Gaussian</a></li>
</ul>
</li>
<li><a href="#plotting-contours-1-P-1">Plotting Contours</a>
<ul>
<li><a href="#derivation-2-P-1">Derivation</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<div id="outline-container-overview-1-P-15" class="outline-2">
<h2 id="overview-1-P-15">Overview</h2>
<div class="outline-text-2" id="text-org256998c">
<ul class="org-ul">
<li>Also called Multivariate Normal Distrubtions (or Multi-Normal)</li>
<li>\(X(\xi) \sim \mathcal{N}(\mu, \Sigma)\),
<ul class="org-ul">
<li>\(X(\xi)\) is a jointly Gaussian random variable, (\(\xi\) is an element of the sample space)</li>
<li>\(\mu \in \mathbb{R}^k\) is the mean. \(\mu = E[X]\) (\(E[]\) is expected value.  I will also drop dependence on the sample space).</li>
<li>\(\Sigma \in \mathbb{R}^{k \times k}\) is the covariance.  \(\Sigma = E[(X - \mu)(X-\mu)^T]\), and is positive definite.</li>
</ul></li>
<li><p>
The probability density function (pdf) for a jointly Gaussian random variable is
</p>
\begin{equation}\label{eq:mvgauss}
f_X(x) = (2\pi)^{-\frac{k}{2}}(\det{\Sigma})^{-\frac{1}{2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-linear-transform-1-P-1" class="outline-2">
<h2 id="linear-transform-1-P-1">Linear Transform</h2>
<div class="outline-text-2" id="text-org8907fcf">
<ul class="org-ul">
<li>Let \(Y = A X + b\), \(X \sim \mathcal{N}(\mu, \Sigma)\), where \(A \in \mathbb{R}^{n \times n}\) is invertable</li>
<li>Then \(Y \sim \mathcal{N}(A \mu + b, A \Sigma A^T)\)</li>
</ul>
</div>
<details id="orgb30e02d"><summary class="header-3">Sketch of a Proof</summary>
<div class="outline-text-3" id="text-orgb30e02d">
<ol class="org-ol">
<li><p>
Use the multi-variable change of variables formula to find the new density:
</p>
\begin{equation}
f_Y(y) = \frac{f_x(x)}{\det A}
\end{equation}</li>

<li>Show that the new density has the same form as  multi-variate Gaussian equation~\eqref{eq:mvgauss}.</li>
</ol>
</div>
</details>
</div>
<div id="outline-container-conditional-distribution-1-P-1" class="outline-2">
<h2 id="conditional-distribution-1-P-1">Conditional Distribution</h2>
<div class="outline-text-2" id="text-orgc788c12">
<ul class="org-ul">
<li>Let \(X \sim \mathcal{N}(\mu, \Sigma)\).</li>
<li><p>
Partition:
</p>
\begin{align}
X &= \begin{bmatrix}X_1 \\ X_2\end{bmatrix} \\
\mu &= \begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}\\
\Sigma &= \begin{bmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}\end{bmatrix}
\end{align}</li>
<li>What is the conditional probability distribution \(f_{X_1}(x_1 | X_2 = a)\)?</li>
<li><p>
Let \(Y \sim X_1 | X_2\) (\(Y\) is a random variable with pdf corresponding to
the conditional probability of \(X_1\) given \(X_2 = a\)):
</p>
\begin{equation}
  Y \sim \mathcal{N}(\mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(a - \mu_2), \Sigma_{11}- \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-random-number-generation-in-c--1-P-1" class="outline-2">
<h2 id="random-number-generation-in-c--1-P-1">Random Number Generation in C++</h2>
<div class="outline-text-2" id="text-org19237b2">
<ul class="org-ul">
<li><p>
If you intend to use the <code>C++</code> standard library for generating random numbers, you can use the following <code>singleton</code> pattern to ensure that
you are only seeding the random number generator once in your process.
</p>
<div class="org-src-container">
<pre class="src src-nil">#include&lt;random&gt;
 std::mt19937 &amp; get_random()
 {
     // static variables inside a function are created once and persist for the remainder of the program
     static std::random_device rd{}; 
     static std::mt19937 mt{rd()};
     // we return a reference to the pseudo-random number genrator object. This is always the
     // same object every time get_random is called
     return mt;
 }

 // To generate a gaussian variable:
 std::normal_distribution&lt;&gt; d(mean, variance);
 d(get_random());

</pre>
</div></li>
<li>Armadillo also has random number generation functions
<ul class="org-ul">
<li>See, for example, <a href="http://arma.sourceforge.net/docs.html#randu_randn_standalone">http://arma.sourceforge.net/docs.html#randu_randn_standalone</a></li>
<li>These random number generators are different then the built-in C++ method</li>
</ul></li>
</ul>
</div>
<div id="outline-container-multivariate-gaussian-2-P-1" class="outline-3">
<h3 id="multivariate-gaussian-2-P-1">Multivariate Gaussian</h3>
<div class="outline-text-3" id="text-orgd368b7f">
<ul class="org-ul">
<li>An algorithm for generating multivariate Gaussian noise, given mean \(m \in \mathbb{R}^n\) and variance \(Q \in \mathbb{R}^{n \times n}\) is as follows:
<ol class="org-ol">
<li>Let \(L\) be the (lower) Cholesky decomposition of \(Q\)
(that is \(L\) is lower triangular such that \(L L^T = Q\).
<ul class="org-ul">
<li>Use the <code>chol()</code> function from armadillo or the <code>llt()</code> function from eigen</li>
</ul></li>
<li>Let \(u \in \mathbb{R}^n\) be generated such that each entry
is independently drawn from a Gaussian distribution with mean 0 and variance 1.</li>
<li>Then, \(v = m + Lu\) will have the desired distribution</li>
</ol></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-plotting-contours-1-P-1" class="outline-2">
<h2 id="plotting-contours-1-P-1">Plotting Contours</h2>
<div class="outline-text-2" id="text-org350824d">
<ul class="org-ul">
<li>The level-sets of a Gaussian distribution (\(f_X(x) = c\)) form ellipsoids.</li>
<li>Plotting the ellipsoid of a Gaussian is an excellent method of visualizing it
(at least in 2 or 3 dimensions).</li>
<li>Diagonalize \(\Sigma\), so that \(\Sigma = P \Lambda P^T\), where the columns of \(P\) are eigenvectors of \(\Sigma\) and
\(\Lambda\) is a diagonal matrix of eigenvalues, where \(P\) are orthogonal matrices \(PP^T = P^TP = I\)
(we can do this since covariance is positive definite).</li>
<li>The lengths of the major axes of the ellipse correspond to the inverse eigenvalues of &Sigma;.</li>
<li>The major axes are rotated by \(P^T\)</li>
</ul>
</div>
<div id="outline-container-derivation-2-P-1" class="outline-3">
<h3 id="derivation-2-P-1">Derivation</h3>
<div class="outline-text-3" id="text-orgaf461a5">
\begin{align}
  c &= (2\pi)^{-\frac{k}{2}}\det{\Sigma}^{-\frac{1}{2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \\
  c' &= e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \\
  c'' &= (x - \mu)^T\Sigma^{-1}(x-\mu) \\
  c'' &= (x - \mu)^T(P\Lambda P^T)^{-1}(x-\mu) \\
  c'' &= (x-\mu)^TP\Lambda^{-1}P^T(x-\mu) \\
  c'' &= y^T \Lambda^{-1} y,
\end{align}
<p>
where
</p>
\begin{align}
c' &=(2\pi)^{\frac{k}{2}}\det{\Sigma}^{1/2} \\
c'' &= 2\log{\frac{1}{c'}}\\
y &= P^T(x-\mu)
\end{align}
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p><p class="outline-2">Author: Matthew Elwin. </p></p>
</div>
</body>
</html>
